# Stellar Abundances and atmospheric Parameters Pipeline (SAPP) Version 1.1#### Authors: Matthew Raymond Gent SAPP is a fully automated python code which culminates different types of observation data (photometry, parallax, asteroseismology and spectroscopy) in order to determine fundamental and atmospheric parameters.This pipeline can be used in two different modes: Full or Lite- Full: Determines parameters from a bayesian framework which calculates and combines 3D probability distribtutions.- Lite: Determines parameters from the Spectroscopy module combined with asteroseismic/surface gravity data## RequirementsSAPP is developed with Python 3.7SAPP requires the following python packages:- numpy- scipy- astropy- matplotlib## Input dataYou must absolutely have stellar evolution models within the directory```bashInput_data/GARSTEC_stellar_evolution_models/evo_tracks```You also need to have a list of the names of these models within ```evo_tracks`` directoryinside the text file ```models_list_new.txt```. The code will read this list and so go throughthe evolution models accordingly.Currently the code can only do evo tracks tailored to Aldo's GARSTEC stellar evolution models.NOTE: the headers of the input models MUST match EXACTLY as described in ```models_header.txt```.## Output data## Source scripts## Overall Structure ## Photometry module## Asteroseismology module## Spectroscopy moduleThe central script required to perform spectroscopy is called SAPP_best_spec_payne_v1p1.py.This is located in the directory:```bashSource_scripts/SAPP_spectroscopy/```The best way to explain how to use spectroscopy is with an example!First, you need to store your observation spectra in the directory```bashInput_data/spectroscopy_observation_data/```You can create your own directory to store one or a list of spectra.Second, you need a Neural Network trained grid of model spectra made by The Payne. The training script is currently not in SAPP's architecture, howeverwe already have a trained file already loaded. This is called ```NN_results_RrelsigN20.npz```and it is located in the directory ```bashInput_data/spectroscopy_model_data/Payne_input_data/```This python binary file contains all you need in order to produce a model spectrain the Gaia ESO Giraffe hr10 format with R=18,000 and wavelength range5329 ```$\AA$``` to 5616 ```$\AA$```.Specifically it contains the weights and bias parameters required for the Neural Network as well as the limits of the grid it was trained on.This training grid consists of an 8 Dimensional parameter space with the following variables ```Teff,logg,[Fe/H],Vmic,Vbrd,[Mg/Fe],[Ti/Fe],[Mn/Fe]```Effective temperature, surface gravity, iron metallicity, microturbulence,velocity broadening (this is effectively vsini), magnesium, titanium and manganese metallicity.There is only two functions you have to have to worry about:1) find_best_val()2) "read_spectra_func()"find_best_val() processes the spectra in several ways, in short it cancontinuum normalise, calculate radial velocity correction, convolve the observationto a lower resolution and match the wavelength scale to the grid of models."read_spectra_func()" is in quote marks as this function can vary in structure,The user should tailor this function specifically the spectra they would liketo analyse.For example, already in the script there are 3 functions to read the followingspectra:- Gaia ESO Giraffe fits files, ```read_fits_GES_GIR_spectra``` - Generic text files, ```read_txt_spectra```- HARPS/UVES fits files ```read_fits_spectra```These all follow similar input which is the variable ```spec_path ```and output ```wavelength,flux,error_med,rv_shift,rv_shift_err,snr_star ```.At the very least these outputs should exist, it is possible for rv_shift and rv_shift_err to be NaN values. This is because sometimes the RV correction is not provided. However the SNR of the star must be known, even a 1st order approximation is good enough (For HARPS/UVES you can set the SNR to be 300/200)respectively.The input arguments for find_best_val() are described within the doc stringof the function. Below we will go through what all these inputs mean and learn what you can do with this module```pythonspec_path = Input_data_path + "spectroscopy_observation_data/18_sco/ADP_18sco_snr396_HARPS_17.707g_error_synth_flag_True_cont_norm_convolved_hr10_.txt"error_map_spec_path = Input_data_path + "spectroscopy_observation_data/18_sco/ADP_18sco_snr396_HARPS_17.707g_error_synth_flag_True_cont_norm_convolved_hr10_.txt"error_mask_index = 0error_mask_recreate_bool = Trueerror_map_use_bool = Truecont_norm_bool = Falserv_shift_recalc = [False,-100,100,0.05]conv_instrument_bool = Falseinput_spec_resolution = 18000numax_iter_bool = Falseniter_numax_MAX = 5numax_input_arr = [3170,159,niter_numax_MAX]recalc_metals_bool = Falsefeh_recalc_fix_bool = Falserecalc_metals_inp = [5770,4.44,0,feh_recalc_fix_bool] ind_spec_arr = [spec_path,\                error_map_spec_path,\                error_mask_index,\                error_mask_recreate_bool,\                error_map_use_bool,\                cont_norm_bool,\                rv_shift_recalc,\                conv_instrument_bool,\                input_spec_resolution,\                numax_iter_bool,\                numax_input_arr,\                recalc_metals_bool,                recalc_metals_inp]find_best_val(ind_spec_arr)```The spectra chosen is a 18 sco HARPS spectra which has been continuum normalised and convolved from R = 118,000 to 18,000 already. Hence why```pythoncont_norm_bool = Falseconv_instrument_bool = False``` If you would like to test out these processes, there is a unprocessed HARPSspectra of 18 sco in the ```spectroscopy_observation_data``` directory.The python code above is the simplest case of processing, finding the best fitparameters of a spectra using an error mask.An error mask is a way of introducing model uncertainties to the fitting routine.Grab the cleanest spectra you have of the star, set the reference parameters and you will create a mask of residuals representing how different the best possiblemodel is from a real observation.For this example you are giving ```error_map_spec_path``` the same path as ```spec_path```,this is because with only one spectra this will have to be your cleanest one. The variable ```error_mask_index``` refers to a list of reference parameters located in directory:```bashInput_data/Reference_data/PLATO_stars_lit_params.txt```Here we have already made a list of reference parameters for our PLATO benchmark stars.The first star in the list is 18 sco, hence why ```error_mask_index = 0``` in thisexample. If you would like to change this then you just have to edit this parameter list.This fits file did not come with a radial velocity value, therefore we must calculate it.The "read_spectra function" will give rv_shift as a NaN value, the code will recognise thisand start the procedure for RV correction.Our RV correction function is called ```rv_cross_corelation_no_fft``` and derivesfrom PyAstronomy's RV Cross Correlation function with some minor changes. To perfom RV correction, you will require at least one "template" to compare your observations to.If your star is a subgiant/dwarf Main-Sequence star similar to our Sun then having a solar template will be good enough. We have already provided a spectral templatein the directory```bashInput_data/spectroscopy_model_data/Sun_model_R5e5_TS_hr10/```We produced a R ~ 500,000 solar model using TurboSpectrum with the hr10 wavelength coverage. Due to the high sampling, this had to be done in sections and therefore we created a function ```stitch_regions``` to stitch the model together.Now all the RV correction function requires is for you to specify the intended Resolution, RV limits and a step size.The parameter ```input_spec_resolution``` is where you define the resolution, it should match the spectral resolution of the trained grid. The array ```rv_shift_recalc``` exists to allow you to decided whether you wouldlike to re-calculate the rv correction or not, as some files do provide thisinformation, you may want to re-calculate it. The first parameter in the array you will set to True or False. The rest are rv_min, rv_max, and drv i.e. the limitsfor the RV correction calculation and step size (in Km/s).This of course will affect the computation time however I normally pick between-100 and 100 Km/s for the limits with 0.05 Km/s as the step size. If you wouldlike to RV correct the spectra as a pre-process, the RV correction function can be found in the directory:```bashSource_scripts/SAPP_tools/```Running this code in the current format will produce the following output```pythonSNR 396BEST fit parameters = [ 5.77216  4.37047  0.02535  1.04010  4.12242  0.02084  0.00929 -0.03453]parameters ERROR upper = [ 0.00630  0.00842  0.00400  0.01466  0.12393  0.00561  0.00711  0.00615]parameters ERROR lower = [ 0.00630  0.00842  0.00400  0.01466  0.12393  0.00561  0.00711  0.00615]```The SNR of the spectra, then the best fit parameters alongside the respective errors. NOTE that the order of the parameters follows the order described abovewith effective temperature, surface gravity and [Fe/H] as the first threeparameters. It was found that fitting Teff in units of K/1000 was easier for the training, hence why you see 5.77216 and not 5772.16.Already we can see that the temperature estimation is good but the logg estimation could be better. This is where the ```numax_iter_bool``` variablecomes into play.Our Lite mode is the use of Spectroscopy with Asteroseismic data. This is how we do it, you must set ```numax_iter_bool = True``` and fill out the parameters ```niter_numax_MAX``` and ```numax_input_arr```. For 18 Sco wehave already filled out the information for you.The parameter ```niter_numax_MAX``` defines the maximum number of iterationsfor the logg refinement. 5 is good enough for most iterations and this goes into the parameter ```numax_input_arr``` where the first two elements arethe ```$\Delta\nu_{max}$``` alongside its respective uncertainty.The process itself is quite simple and can be understood from the code. In shortthe code first makes its best guess of all the parameters, which will result inthe exact same output as above. It then uses the effective temperature combinedwith ```$\Delta\nu_{max}$``` to calculate a new logg value via the scaling relationship below: ```INPUT SCALING RELATIONSHIP.```This logg estimate is then fixed while all the other parameters are re-calculated, this gives a new effective temperature and so a new logg valuecan be calculated. This iterative process runs for as many loops as the usergives it.Once the process is finished, the code finds where the process has a temperaturevalue oscillating around some central value within 10 K. The process would never fully converge, however once we get to the oscillating region we considerit finished.```INSERT FIGURE HERE OF OSCILLATION EXAMPLE.```The output of the code run should now look like this:```pythonSNR 396BEST fit parameters = [ 5.77216  4.37047  0.02535  1.04010  4.12242  0.02084  0.00929 -0.03453]parameters ERROR upper = [ 0.00630  0.00842  0.00400  0.01466  0.12393  0.00561  0.00711  0.00615]parameters ERROR lower = [ 0.00630  0.00842  0.00400  0.01466  0.12393  0.00561  0.00711  0.00615]===== niter 1 ============================================BEST fit parameters = [ 5.82266  4.44817  0.04979  1.03245  4.31887 -0.00184  0.02546 -0.02194]parameters ERROR upper = [ 0.00633  0.00829  0.00399  0.01496  0.12429  0.00550  0.00728  0.00613]parameters ERROR lower = [ 0.00633  0.00829  0.00399  0.01496  0.12429  0.00550  0.00728  0.00613]=========================================================== niter 2 ============================================BEST fit parameters = [ 5.82235  4.45006  0.04869  1.02953  3.98030 -0.00097  0.02712 -0.01814]parameters ERROR upper = [ 0.00632  0.00831  0.00398  0.01493  0.12282  0.00547  0.00725  0.00612]parameters ERROR lower = [ 0.00632  0.00831  0.00398  0.01493  0.12282  0.00547  0.00725  0.00612]=========================================================== niter 3 ============================================BEST fit parameters = [ 5.82512  4.45005  0.05075  1.03767  4.31245 -0.00299  0.02726 -0.02182]parameters ERROR upper = [ 0.00633  0.00829  0.00399  0.01494  0.12424  0.00550  0.00727  0.00613]parameters ERROR lower = [ 0.00633  0.00829  0.00399  0.01494  0.12424  0.00550  0.00727  0.00613]=========================================================== niter 4 ============================================BEST fit parameters = [ 5.82194  4.45015  0.04777  1.03673  3.92680 -0.00157  0.02753 -0.01746]parameters ERROR upper = [ 0.00631  0.00826  0.00398  0.01485  0.13033  0.00547  0.00721  0.00607]parameters ERROR lower = [ 0.00631  0.00826  0.00398  0.01485  0.13033  0.00547  0.00721  0.00607]=========================================================== niter 5 ============================================BEST fit parameters = [ 5.82717  4.45003  0.04841  1.07876  4.53197 -0.00086  0.03006 -0.02275]parameters ERROR upper = [ 0.00633  0.00831  0.00399  0.01486  0.12411  0.00552  0.00725  0.00611]parameters ERROR lower = [ 0.00633  0.00831  0.00399  0.01486  0.12411  0.00552  0.00725  0.00611]=========================================================== niter 6 ============================================BEST fit parameters = [ 5.82362  4.45023  0.04914  1.03827  4.17439 -0.00137  0.02844 -0.02062]parameters ERROR upper = [ 0.00633  0.00829  0.00399  0.01492  0.12424  0.00550  0.00726  0.00613]parameters ERROR lower = [ 0.00633  0.00829  0.00399  0.01492  0.12424  0.00550  0.00726  0.00613]======================================================BEST fit parameters = [ 5.82512  4.45005  0.05075  1.03767  4.31245 -0.00299  0.02726 -0.02182]parameters ERROR upper = [ 0.00662  0.00829  0.00414  0.02328  0.25228  0.00556  0.00735  0.00645]parameters ERROR lower = [ 0.00662  0.00829  0.00414  0.02328  0.25228  0.00556  0.00735  0.00645]```The last way to use the code directly relates to how it is used in the Full mode which uses the Bayesian framework.This is concerning the booleans ```recalc_metals_bool``` and ```feh_recalc_fix_bool```.The Full mode only explores the first 3 dimensions of the spectroscopy PDF whilekeeping the other 5 parameters at their best fit solution. Once we combine thedifferent PDFs alongside priors to create an overall posterior the rest of theparameters need to be recalculated based on the new best fit solution. Therefore ```recalc_metals_bool = True``` will tell the code to look at the parameters given in the array ```recalc_metals_inp``` and fix the first two/threeparameters while re-calculating the rest. The effective temperature and logg are always fixed, however I give the the user the choice to not fix [Fe/H] asthis is an abundance and you might want to self-consistently re-calculate them all.The inputs we have chosen are the Solar values, as 18 sco is known as the Solartwin. NOTE by setting ```recalc_metals_bool = True``` this will negate the nu_maxiterative process since both Teff and logg are being fixed. The output of the code in this mode with ```feh_recalc_fix_bool = False``` is shown below:```pythonSNR 396BEST fit parameters = [ 5.77030  4.43961  0.01079  1.05678  4.27973 -0.00909  0.02424 -0.02365]parameters ERROR upper = [ 0.00637  0.00828  0.00420  0.01495  0.12486  0.00554  0.00713  0.00617]parameters ERROR lower = [ 0.00637  0.00828  0.00420  0.01495  0.12486  0.00554  0.00713  0.00617]```And what follows is if ```feh_recalc_fix_bool = True```:```pythonSNR 396BEST fit parameters = [ 5.77030  4.43961  0.00038  1.09698  4.12245  0.00072  0.03523 -0.00777]parameters ERROR upper = [ 0.00645  0.00843  0.00419  0.01484  0.12359  0.00551  0.00723  0.00618]parameters ERROR lower = [ 0.00645  0.00843  0.00419  0.01484  0.12359  0.00551  0.00723  0.00618]```#### Should there be more figures in this section?#### It shouldn't be a thesis, what is too much?## Bayesian Framework If you would like to run the FULL bayesian scheme then you should look at the script located in the following directory:```bashSource_scripts/main_FULL.py```Inside this script you will find only three functions, of which only one concernsyou with respect to input, ```main_func()```.Firstly, you need to understand the inputs for the full mode, these are split upinto 3 sections- Photometry data- Asteroseismology data- Spectroscopy dataThese are loaded in from the ```Input_data``` directory and the headers are definedwithin the main_FULL.py script.You should see something like this```pythonInput_phot_data = np.loadtxt('../Input_data/photometry_asteroseismology_observation_data/PLATO_benchmark_stars/PLATO_bmk_phot_data/PLATO_photometry.csv',delimiter = ',',dtype=str)"""0 ; source_id (Literature name of the star)1 ; G2 ; eG3 ; Bp4 ; eBp5 ; Rp6 ; eRp7 ; Gaia Parallax (mas)8 ; eGaia Parallax (mas)9 ; AG [mag], Gaia Extinction10 ; Gaia DR2 ids11 ; B (Johnson filter) [mag]12 ; eB (Johnson filter) [mag]13 ; V (Johnson filter) [mag]14 ; eV (Johnson filter) [mag]15 ; H (2MASS filter) [mag]16 ; eH (2MASS filter) [mag]17 ; J (2MASS filter) [mag]18 ; eJ (2MASS filter) [mag]19 ; K (2MASS filter) [mag]20 ; eK (2MASS filter) [mag]21 ; SIMBAD Parallax (mas)22 ; eSIMBAD Parallax (mas)23 ; E(B-V), reddening value from stilism 	24 ; E(B-V)_upp, upper reddening uncertainty from stilism 	25 ; E(B-V)_low, lower reddening uncertainty from stilism 	"""Input_ast_data = np.loadtxt('../Input_data/photometry_asteroseismology_observation_data/PLATO_benchmark_stars/Seismology_calculation/PLATO_stars_seism_copy.txt',delimiter = ',',dtype=str)"""0 ; source_id (Literature name of the star)1 ; nu_max, maximum frequency [microHz]2 ; err nu_max pos, upper uncertainty3 ; err nu_max neg, lower uncertainty4 ; d_nu, large separation frequency [microHz]5 ; err d_nu pos, upper uncertainty [microHz]6 ; err d_nu neg, lower uncertainty [microHz]"""Input_spec_data = np.loadtxt('../Input_data/spectroscopy_observation_data/spectra_list.txt',delimiter=',',dtype=str)"""0 ; source_id (Literature name of the star)1 ; spectra type (e.g. HARPS or UVES_cont_norm_convolved)2 ; spectra path"""```The observation data needs to match these headers EXACTLY.Once the input data is loaded you need to make some decisons based on how the spectra are processed (just like in the spectroscopy section).```pythonerror_mask_recreate_bool = Trueerror_map_use_bool = Truecont_norm_bool = Falserv_shift_recalc = [False,-100,100,0.05]conv_instrument_bool = Falseinput_spec_resolution = 18000numax_iter_bool = Trueniter_numax_MAX = 5recalc_metals_bool = Falsefeh_recalc_fix_bool = False```To understand these settings better, read the spectroscopy documentation above.The only input for ```main_func()``` is ```inp_index```, which refers to the index of a list of stars organised by your input data.NOTE: all input data including reference data should be in the same order (by rows)with the exact same identifier, its how cross correlation works. Inside ```main_func()``` you decide on the resolution of the core parameter space,Teff, logg and [Fe/H]. This will dictate how discrete you want to interpolate the photometry/asteroseismology space and how fine of a grid you sample in the spectroscopy space.Changing this will not necessarily affect computation time with respect to interpolationunless the resolution is really small. However it will affect spectroscopy computation timeas a grid needs to be sampled and built. 100x100x100 grid size would take around 30 minutes for spectroscopy, but that resolutionis unnecesary, by choosing 50 K for Teff, 0.05 dex for both logg and [Fe/H], spectroscopygrid will only take around 6/7 minutes.What constrains the computation time for interpolation is the next two variables ```asteroseismolgy_logg_cut``` and ```photometry_non_gaia_colour_prob_cut```.The former cuts the logg space centering around the best logg value estimated from asteroseismology PDFwithin a tolerance you choose, for example setting it to 0.5 dex means that all PDFs in logg dimension will be cut such that their logg is within 0.5 dex of the most probable logg from asteroseismology.The latter cuts the temperature dimension by using photometry colour PDF as a proxy.Applying a probability threshold to the non-gaia photometry colour PDF effectively constrains the temperature. I typically choose around 10$^-{10}$ as any probability lower than thiswould not likely contain the best fit values, its effectively zero. You will see the impact of these two parameters when the function ```post_cut_limits``` is run,as it will print the number of points per axes it will interpolate each time the probability spaceis "cut" somehow. These two values I picked can reduce the number by an order of magnitude.The next set of inputs concerns spectroscopy                ```pythonspec_obs_number = 0spec_type = Input_spec_data[:,1][correlation_arr[inp_index][spec_obs_number]]\                + "_" + str(spec_obs_number)spec_path = Input_spec_data[:,2][correlation_arr[inp_index][spec_obs_number]]error_map_spec_path = Input_spec_data[:,2][correlation_arr[inp_index][spec_obs_number]]error_mask_index = inp_indexnu_max = Input_ast_data[inp_index][1]nu_max_err = Input_ast_data[inp_index][2]numax_input_arr = [float(nu_max),float(nu_max_err),niter_numax_MAX]recalc_metals_inp = [5770,4.44,0,feh_recalc_fix_bool] ```You shouldn't need to edit any of these, unless you would like to fix the first 3 parameters in spectroscopy, if you want to do this, pick what values you want in the variable ```recalc_metals_inp``` and set ```feh_recalc_fix_bool=True``` outside the ```main_func()```.For now keep the parameter ```spec_obs_number``` at zero as I am yet to find a wayto cleanly deal with multiple observations within a large loop run of a whole set of stars.You can change it to another value if you have another observation and it will run, just note thatrunning this in a loop over all the observations may cause you to re-calculate steps which isa waste of computation time.Running the code in this state will run 6 main modules- Photometry and Asteroseismology- Cutting parameter space and grid size- Spectroscopy grid- Interpolation - Decomposition of interpolated grids (from multi-D grids to something readable i.e. 1D columns)- Bayesian scheme which combines grids and calculates best fit parameters with errors.Once you have run it entirey, you can of course block out some of these steps to save re-calculation, for example if you want to recalc spectroscopy but photometry already exists,block everything out except spectroscopy.NOTE: The Bayesian scheme creates figures which are of publication quality (in theory) and so this module can take around 10 minutes but you can see the figures pop up in ```BashOutput_figures/Bayesian_PDF/```as they are being created. If you do not want to look at the graphs/don't careyou can set ```savefig_bayes_bool = False```. The output data  from this module will be saved in ```BashOutput_data/Stars_Lhood_combined_spec_phot/18sco/best_fit_params_18sco_metals.txtOutput_data/Stars_Lhood_combined_spec_phot/18sco/best_fit_params_errors_18sco_metals.txt```18sco is an example. There are headers in the text files describing what each column reperesents.The rows of these text files represent best fit parameters from different PDF grids/Combined gridsThe errors and best fit parameters are in separate files.Now, try to run the full code! If there are any issues, please contact me at ```gent@mpia.de```